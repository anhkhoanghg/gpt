{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "num_dow = random.randint(1, 7)\n",
    "print(num_dow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3, 2, 1]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = []\n",
    "while num_dow>0:\n",
    "    a.append(num_dow)\n",
    "    num_dow -=1\n",
    "    \n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18:00:00&7:00:00\n"
     ]
    }
   ],
   "source": [
    "daily = [\"18:00:00\", \"7:00:00\"]\n",
    "\n",
    "# Use the join() method to concatenate the elements with '&'\n",
    "timer = '&'.join(daily)\n",
    "\n",
    "print(timer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<prompt>',\n",
       " '</prompt>',\n",
       " '<task>',\n",
       " '</task>',\n",
       " '<sum>',\n",
       " '<cate>',\n",
       " '<prio>',\n",
       " '<diff>',\n",
       " '<imp>',\n",
       " '<freq>',\n",
       " '<exp_min>',\n",
       " '<totd>',\n",
       " '<spec_time>',\n",
       " '<dow>',\n",
       " '<day>',\n",
       " '<month>',\n",
       " '<no_date>',\n",
       " '<no_week>',\n",
       " '<no_month>',\n",
       " '<daily>',\n",
       " '<weekly>']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "def loadTagToken(token_annotation_path):\n",
    "    listToken = []\n",
    "    with open(token_annotation_path, 'r', encoding=\"utf-8\") as file:\n",
    "        data = json.load(file)\n",
    "        level_2_tokens = data[\"level_2\"][\"token\"]\n",
    "        level_3_tokens = data[\"level_3\"][\"3.1\"][\"token\"]\n",
    "        for i in range(len(level_2_tokens)):\n",
    "            listToken.append(level_2_tokens[i][\"value\"])\n",
    "        for i in range(len(level_3_tokens)):\n",
    "            listToken.append(level_3_tokens[i][\"value\"])\n",
    "    \n",
    "    return listToken\n",
    "\n",
    "x= loadTagToken('./token_annotation.json')\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "hour = 6\n",
    "time_ranges = {\n",
    "    \"cc\": range(0, 3),\n",
    "    \"db\": range(3,12)\n",
    "}\n",
    "x = \"\"\n",
    "for key, value in time_ranges.items():\n",
    "    if hour in value:\n",
    "        x= key\n",
    "        break\n",
    "    else:\n",
    "        x = \"deo co j\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-15T11:25:14.221963Z",
     "start_time": "2023-11-15T11:25:14.153476Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "36252"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Assuming your JSON-like structure is stored in a file named 'data.json'\n",
    "file_path = './data/prompt-target/full_data.json'\n",
    "\n",
    "# Read the JSON data from the file\n",
    "with open(file_path, 'r') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# Check the number of entries in the JSON file\n",
    "number_of_entries = len(data)\n",
    "number_of_entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dee41c163a25466396377f3c167c1cd5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be9509d831bb484ba6e01bb539e36536",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "448cad3c327849e48cc564776b6543cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "NameError",
     "evalue": "name 'num_categories' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 7\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[39m# Load pre-trained BERT model and tokenizer\u001b[39;00m\n\u001b[0;32m      6\u001b[0m tokenizer \u001b[39m=\u001b[39m BertTokenizer\u001b[39m.\u001b[39mfrom_pretrained(\u001b[39m'\u001b[39m\u001b[39mbert-base-uncased\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m----> 7\u001b[0m model \u001b[39m=\u001b[39m BertForSequenceClassification\u001b[39m.\u001b[39mfrom_pretrained(\u001b[39m'\u001b[39m\u001b[39mbert-base-uncased\u001b[39m\u001b[39m'\u001b[39m, num_labels\u001b[39m=\u001b[39mnum_categories)\n\u001b[0;32m      9\u001b[0m \u001b[39m# Input text\u001b[39;00m\n\u001b[0;32m     10\u001b[0m input_text \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mgo to work\u001b[39m\u001b[39m\"\u001b[39m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'num_categories' is not defined"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "import torch\n",
    "from torch.nn.functional import softmax\n",
    "\n",
    "# Load pre-trained BERT model and tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=num_categories)\n",
    "\n",
    "# Input text\n",
    "input_text = \"go to work\"\n",
    "\n",
    "# Tokenize and convert to tensor\n",
    "inputs = tokenizer(input_text, return_tensors=\"pt\")\n",
    "\n",
    "# Make prediction\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "# Get probabilities for each category using softmax\n",
    "probabilities = softmax(outputs.logits, dim=1)\n",
    "\n",
    "# Get the predicted category\n",
    "predicted_category_index = torch.argmax(probabilities).item()\n",
    "\n",
    "# Print the predicted category\n",
    "print(\"Predicted Category:\", predicted_category_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT2LMHeadModel(\n",
      "  (transformer): GPT2Model(\n",
      "    (wte): Embedding(50281, 768)\n",
      "    (wpe): Embedding(1024, 768)\n",
      "    (drop): Dropout(p=0.1, inplace=False)\n",
      "    (h): ModuleList(\n",
      "      (0-11): 12 x GPT2Block(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPT2Attention(\n",
      "          (c_attn): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): GPT2MLP(\n",
      "          (c_fc): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (act): NewGELUActivation()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=768, out_features=50281, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "import torch\n",
    "import json\n",
    "import re\n",
    "\n",
    "\n",
    "def loadTagToken(token_annotation_path):\n",
    "    listToken = []\n",
    "    with open(token_annotation_path, 'r', encoding=\"utf-8\") as file:\n",
    "        data = json.load(file)\n",
    "        level_2_tokens = data[\"level_2\"][\"token\"]\n",
    "        level_3_tokens = data[\"level_3\"][\"3.1\"][\"token\"]\n",
    "        for i in range(len(level_2_tokens)):\n",
    "            listToken.append(level_2_tokens[i][\"value\"])\n",
    "        for i in range(len(level_3_tokens)):\n",
    "            listToken.append(level_3_tokens[i][\"value\"])\n",
    "    \n",
    "    return listToken\n",
    "\n",
    "custom_tokens = loadTagToken('./token_annotation.json')\n",
    "\n",
    "special_tokens = {\n",
    "    \"pad_token\": \"<pad>\",\n",
    "    \"bos_token\": \"<s>\",\n",
    "    \"eos_token\": \"</s>\",\n",
    "    \"additional_special_tokens\": custom_tokens\n",
    "}\n",
    "\n",
    "# Load tokenizer with custom vocabulary and tokens\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "tokenizer.add_special_tokens(special_tokens)\n",
    "\n",
    "# Load GPT-2 model\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "# Load trained model state\n",
    "model.load_state_dict(torch.load(\"./modelPred/model/checkpoint-1812600/pytorch_model.bin\", map_location=torch.device('cuda')))\n",
    "# model.load_state_dict(torch.load(\"./checkpoint-650/pytorch_model.bin\", map_location=torch.device('cuda')))\n",
    "model.eval()\n",
    "\n",
    "print(model.eval())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "stock_pred",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "4d6c00a200ee523965bd64971563c2527a5168161590ce1dc255c51766d02a15"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
