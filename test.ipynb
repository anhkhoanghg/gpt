{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "num_dow = random.randint(1, 7)\n",
    "print(num_dow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3, 2, 1]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = []\n",
    "while num_dow>0:\n",
    "    a.append(num_dow)\n",
    "    num_dow -=1\n",
    "    \n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18:00:00&7:00:00\n"
     ]
    }
   ],
   "source": [
    "daily = [\"18:00:00\", \"7:00:00\"]\n",
    "\n",
    "# Use the join() method to concatenate the elements with '&'\n",
    "timer = '&'.join(daily)\n",
    "\n",
    "print(timer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<prompt>',\n",
       " '</prompt>',\n",
       " '<task>',\n",
       " '</task>',\n",
       " '<sum>',\n",
       " '<cate>',\n",
       " '<prio>',\n",
       " '<diff>',\n",
       " '<imp>',\n",
       " '<freq>',\n",
       " '<exp_min>',\n",
       " '<totd>',\n",
       " '<spec_time>',\n",
       " '<dow>',\n",
       " '<day>',\n",
       " '<month>',\n",
       " '<no_date>',\n",
       " '<no_week>',\n",
       " '<no_month>',\n",
       " '<daily>',\n",
       " '<weekly>']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "def loadTagToken(token_annotation_path):\n",
    "    listToken = []\n",
    "    with open(token_annotation_path, 'r', encoding=\"utf-8\") as file:\n",
    "        data = json.load(file)\n",
    "        level_2_tokens = data[\"level_2\"][\"token\"]\n",
    "        level_3_tokens = data[\"level_3\"][\"3.1\"][\"token\"]\n",
    "        for i in range(len(level_2_tokens)):\n",
    "            listToken.append(level_2_tokens[i][\"value\"])\n",
    "        for i in range(len(level_3_tokens)):\n",
    "            listToken.append(level_3_tokens[i][\"value\"])\n",
    "    \n",
    "    return listToken\n",
    "\n",
    "x= loadTagToken('./token_annotation.json')\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "hour = 6\n",
    "time_ranges = {\n",
    "    \"cc\": range(0, 3),\n",
    "    \"db\": range(3,12)\n",
    "}\n",
    "x = \"\"\n",
    "for key, value in time_ranges.items():\n",
    "    if hour in value:\n",
    "        x= key\n",
    "        break\n",
    "    else:\n",
    "        x = \"deo co j\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-15T11:25:14.221963Z",
     "start_time": "2023-11-15T11:25:14.153476Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "36252"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Assuming your JSON-like structure is stored in a file named 'data.json'\n",
    "file_path = './data/prompt-target/full_data.json'\n",
    "\n",
    "# Read the JSON data from the file\n",
    "with open(file_path, 'r') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# Check the number of entries in the JSON file\n",
    "number_of_entries = len(data)\n",
    "number_of_entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import json\n",
    "# Load the file\n",
    "pt_file = torch.load(\"./modelPred/model/checkpoint-1812600/optimizer.pt\")\n",
    "def convert_tensors(obj):\n",
    "    if isinstance(obj, torch.Tensor):\n",
    "        # Move the tensor to CPU before converting to NumPy\n",
    "        return obj.cpu().numpy().tolist()\n",
    "    return obj\n",
    "\n",
    "# Save the JSON to a file\n",
    "with open('./modelPred/model/checkpoint-1812600/optim.json', 'w') as json_file:\n",
    "    json.dump(pt_file, json_file, indent=2, default=convert_tensors)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT2LMHeadModel(\n",
      "  (transformer): GPT2Model(\n",
      "    (wte): Embedding(50281, 768)\n",
      "    (wpe): Embedding(1024, 768)\n",
      "    (drop): Dropout(p=0.1, inplace=False)\n",
      "    (h): ModuleList(\n",
      "      (0-11): 12 x GPT2Block(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPT2Attention(\n",
      "          (c_attn): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): GPT2MLP(\n",
      "          (c_fc): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (act): NewGELUActivation()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=768, out_features=50281, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "import torch\n",
    "import json\n",
    "import re\n",
    "\n",
    "\n",
    "def loadTagToken(token_annotation_path):\n",
    "    listToken = []\n",
    "    with open(token_annotation_path, 'r', encoding=\"utf-8\") as file:\n",
    "        data = json.load(file)\n",
    "        level_2_tokens = data[\"level_2\"][\"token\"]\n",
    "        level_3_tokens = data[\"level_3\"][\"3.1\"][\"token\"]\n",
    "        for i in range(len(level_2_tokens)):\n",
    "            listToken.append(level_2_tokens[i][\"value\"])\n",
    "        for i in range(len(level_3_tokens)):\n",
    "            listToken.append(level_3_tokens[i][\"value\"])\n",
    "    \n",
    "    return listToken\n",
    "\n",
    "custom_tokens = loadTagToken('./token_annotation.json')\n",
    "\n",
    "special_tokens = {\n",
    "    \"pad_token\": \"<pad>\",\n",
    "    \"bos_token\": \"<s>\",\n",
    "    \"eos_token\": \"</s>\",\n",
    "    \"additional_special_tokens\": custom_tokens\n",
    "}\n",
    "\n",
    "# Load tokenizer with custom vocabulary and tokens\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "tokenizer.add_special_tokens(special_tokens)\n",
    "\n",
    "# Load GPT-2 model\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "# Load trained model state\n",
    "model.load_state_dict(torch.load(\"./modelPred/model/checkpoint-1812600/pytorch_model.bin\", map_location=torch.device('cuda')))\n",
    "# model.load_state_dict(torch.load(\"./checkpoint-650/pytorch_model.bin\", map_location=torch.device('cuda')))\n",
    "model.eval()\n",
    "\n",
    "print(model.eval())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total entries: 36252\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "\n",
    "json_file_path = './data/prompt-target/full_data.json'\n",
    "\n",
    "with open(json_file_path, 'r') as json_file:\n",
    "    data = json.load(json_file)\n",
    "\n",
    "    num_entries = len(data)\n",
    "\n",
    "    print(f'Total entries: {num_entries}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "stock_pred",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "4d6c00a200ee523965bd64971563c2527a5168161590ce1dc255c51766d02a15"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
